import copy

import numpy
from plangym import AtariEnvironment, ParallelEnvironment

from fragile.core.env import DiscreteEnv
from fragile.core.states import StatesEnv, StatesModel
from fragile.core.swarm import Swarm
from fragile.core.utils import StateDict


def get_plangym_env(swarm: Swarm) -> AtariEnvironment:
    """Return the :class:`plangym.AtariEnvironment` of the target Swarm."""
    if not isinstance(swarm.env, AtariEnv):
        raise TypeError("env needs to be an instanc of AtariEnv.")
    plangym_env = swarm.env._env
    if isinstance(plangym_env, ParallelEnvironment):
        return plangym_env._env
    else:
        return plangym_env


class AtariEnv(DiscreteEnv):
    """The AtariEnv acts as an interface with `plangym.AtariEnvironment`.

    It can interact with any Atari environment that follows the interface of ``plangym``.
    """

    STATE_CLASS = StatesEnv

    def get_params_dict(self) -> StateDict:
        """Return a dictionary containing the param_dict to build an instance \
        of States that can handle all the data generated by the environment.
        """
        super_params = super(AtariEnv, self).get_params_dict()
        params = {"game_ends": {"dtype": numpy.bool_}}
        params.update(super_params)
        return params

    def step(self, model_states: StatesModel, env_states: StatesEnv) -> StatesEnv:
        """
        Set the environment to the target states by applying the specified \
        actions an arbitrary number of time steps.

        Args:
            model_states: States representing the data to be used to act on the environment.
            env_states: States representing the data to be set in the environment.

        Returns:
            States containing the information that describes the new state of the Environment.

        """
        actions = model_states.actions.astype(numpy.int32)
        n_repeat_actions = model_states.dt if hasattr(model_states, "dt") else 1
        new_states, observs, rewards, ends, infos = self._env.step_batch(
            actions=actions, states=env_states.states, n_repeat_action=n_repeat_actions
        )
        game_ends = [inf.get("game_end", False) for inf in infos]

        new_state = self.states_from_data(
            states=new_states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=len(actions),
            game_ends=game_ends,
        )
        return new_state

    def reset(self, batch_size: int = 1, **kwargs) -> StatesEnv:
        """
        Reset the environment to the start of a new episode and returns a new \
        :class:`StatesEnv` instance describing the state of the :class:`AtariEnvironment`.

        Args:
            batch_size: Number of walkers of the returned state.
            **kwargs: Ignored. This environment resets without using any external data.

        Returns:
            :class:`StatesEnv` instance describing the state of the Environment. \
            The first dimension of the data tensors (number of walkers) will be \
            equal to batch_size.

        """
        state, obs = self._env.reset()
        states = numpy.array([copy.deepcopy(state) for _ in range(batch_size)])
        observs = numpy.array([copy.deepcopy(obs) for _ in range(batch_size)])
        rewards = numpy.zeros(batch_size, dtype=numpy.float32)
        ends = numpy.zeros(batch_size, dtype=numpy.bool_)
        game_ends = numpy.zeros(batch_size, dtype=numpy.bool_)
        new_states = self.states_from_data(
            states=states,
            observs=observs,
            rewards=rewards,
            ends=ends,
            batch_size=batch_size,
            game_ends=game_ends,
        )
        return new_states
